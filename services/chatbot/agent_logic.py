import os
from langchain_groq import ChatGroq
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

template = """AÅŸaÄŸÄ±daki dÃ¶kÃ¼manlara dayanarak soruyu profesyonel bir lojistik uzmanÄ± gibi yanÄ±tla.

KURALLAR:
1. DÄ°L UYUMU: Soruyu hangi dilde aldÄ±ysan (Ä°ngilizce, TÃ¼rkÃ§e vb.) o dilde yanÄ±t ver. 
2. FORMAT: "GiriÅŸ", "Nedenler" gibi baÅŸlÄ±klar veya madde iÅŸaretleri ASLA kullanma.
3. AKICILIK: CevabÄ± tek bir paragraf veya en fazla iki kÄ±sa paragraf ÅŸeklinde, akÄ±cÄ± bir metin olarak yaz.
4. Ä°Ã‡ERÄ°K: DÃ¶kÃ¼manlardaki bilgileri kullanarak doÄŸrudan ve aÃ§Ä±klayÄ±cÄ± bir cevap sun. Gereksiz dolgu cÃ¼mlelerinden kaÃ§Ä±n.

DÃ¶kÃ¼manlar: {context}
Soru: {question}
YanÄ±t (Soruyla aynÄ± dilde):"""

PROMPT = PromptTemplate(template=template, input_variables=["context", "question"])


def create_logistics_agent():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_path = os.path.join(current_dir, "data")

    try:
        # 1. DokÃ¼manlarÄ± YÃ¼kle
        loader = DirectoryLoader(data_path, glob="./*.txt", loader_cls=TextLoader)
        documents = loader.load()
        print(f"âœ… {len(documents)} dÃ¶kÃ¼man yÃ¼klendi.")

        # 2. Ãœcretsiz ve Stabil Embedding Modeli (HuggingFace)
        # Gemini hatasÄ± almamak iÃ§in bunu lokalde Ã§alÄ±ÅŸtÄ±rÄ±yoruz
        embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        
        # 3. VektÃ¶r VeritabanÄ±
        vectorstore = FAISS.from_documents(documents, embeddings)
        print("âœ… VektÃ¶r veritabanÄ± hazÄ±r.")

        # 4. Groq LLM YapÄ±landÄ±rmasÄ±
        # Groq API Key'ini .env dosyana GROQ_API_KEY olarak eklemeyi unutma
        llm = ChatGroq(
            temperature=0, 
            model_name="llama-3.3-70b-versatile", # Ãœcretsiz ve Ã§ok hÄ±zlÄ± model
            groq_api_key=os.getenv("GROQ_API_KEY"),
            max_retries=2
        )

        # 5. RAG Chain
        rag_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(),
            chain_type_kwargs={"prompt":PROMPT}
        )
        print("ğŸš€ Groq RAG AjanÄ± HazÄ±r!")
        return rag_chain

    except Exception as e:
        print(f"ğŸ’¥ Groq Ajan HatasÄ±: {str(e)}")
        return None